[TOC](ch00_overview.md)

# Chapter 3: Schema Design and Data Modeling

> "Bad programmers worry about the code. Good programmers worry about data structures and their relationships." â€” Linus Torvalds

Effective data modeling is crucial to any complex software system, and our LangGraph research assistant is no exception. This chapter explores how we use Pydantic and TypedDict to create a robust type system that structures data flow through our application, ensuring consistency and type safety across complex workflows.

## Core Data Modeling with Pydantic

Pydantic is a data validation library that allows us to define data models with type annotations, providing runtime validation and automatic error handling. In our research assistant, we use Pydantic for several key models:

```python
from pydantic import BaseModel, Field
from typing import Annotated, List
from typing_extensions import TypedDict

class Analyst(BaseModel):
    affiliation: str = Field(
        description="Primary affiliation of the analyst.",
    )
    name: str = Field(
        description="Name of the analyst."
    )
    role: str = Field(
        description="Role of the analyst in the context of the topic.",
    )
    description: str = Field(
        description="Description of the analyst focus, concerns, and motives.",
    )
    @property
    def persona(self) -> str:
        return f"Name: {self.name}\nRole: {self.role}\nAffiliation: {self.affiliation}\nDescription: {self.description}\n"

class Perspectives(BaseModel):
    analysts: List[Analyst] = Field(
        description="Comprehensive list of analysts with their roles and affiliations.",
    )

class SearchQuery(BaseModel):
    search_query: str = Field(
        None, 
        description="Search query for retrieval."
    )
```

These Pydantic models serve several important purposes:

1. **Type validation**: Ensuring data conforms to expected types
2. **Self-documentation**: Field descriptions document the purpose of each field
3. **Structured output**: Used with LangChain's structured output functionality
4. **Helper methods**: Methods like `persona()` format data for use in prompts

### Field Descriptions for LLM Guidance

Note the use of the `Field` constructor with `description` parameters. These descriptions serve as guidance for the LLM when we use structured output:

```python
# Enforce structured output
structured_llm = llm.with_structured_output(Perspectives)
```

When `structured_llm` is invoked, it uses these descriptions to guide the LLM in generating properly structured outputs, helping ensure that the model understands what each field should contain.

### Derived Properties

The `Analyst` model includes a derived property `persona` that formats the analyst's information for use in prompts:

```python
@property
def persona(self) -> str:
    return f"Name: {self.name}\nRole: {self.role}\nAffiliation: {self.affiliation}\nDescription: {self.description}\n"
```

This property is used in the question generation prompt:

```python
system_message = question_instructions.format(goals=analyst.persona)
```

By encapsulating this formatting logic in the model itself, we keep our code DRY and ensure consistent formatting throughout the application.

## Building a State Management System

While Pydantic models are excellent for structured data with validation, LangGraph uses TypedDict for its state types. TypedDict provides a way to define dictionary types with specific keys and value types:

```python
class GenerateAnalystsState(TypedDict):
    topic: str # Research topic
    max_analysts: int # Number of analysts
    human_analyst_feedback: str # Human feedback
    analysts: List[Analyst] # Analyst asking questions

class ResearchGraphState(TypedDict):
    topic: str # Research topic
    max_analysts: int # Number of analysts
    human_analyst_feedback: str # Human feedback
    analysts: List[Analyst] # Analyst asking questions
    sections: Annotated[list, operator.add] # Send() API key
    introduction: str # Introduction for the final report
    content: str # Content for the final report
    conclusion: str # Conclusion for the final report
    final_report: str # Final report
```

TypedDict types don't provide runtime validation like Pydantic, but they do provide type checking in IDEs and static type checkers, helping catch errors during development.

## Understanding the Type Hierarchy

Our system uses three main state types, creating a hierarchy of states that manage different aspects of the research process:

1. **GenerateAnalystsState**: Used during the analyst generation phase
2. **InterviewState**: Used for individual interviews
3. **ResearchGraphState**: Used for the overall research process

### GenerateAnalystsState

```python
class GenerateAnalystsState(TypedDict):
    topic: str # Research topic
    max_analysts: int # Number of analysts
    human_analyst_feedback: str # Human feedback
    analysts: List[Analyst] # Analyst asking questions
```

This state type tracks information needed for generating analyst personas:
- The research topic
- The maximum number of analysts to generate
- Optional human feedback on the analysts
- The generated analysts themselves

### InterviewState

```python
class InterviewState(MessagesState):
    max_num_turns: int # Number turns of conversation
    context: Annotated[list, operator.add] # Source docs
    analyst: Analyst # Analyst asking questions
    interview: str # Interview transcript
    sections: list # Final key we duplicate in outer state for Send() API
```

This state type extends LangGraph's `MessagesState`, adding fields for:
- The maximum number of conversation turns
- Accumulated context from information retrieval
- The analyst conducting the interview
- The complete interview transcript
- Sections generated from the interview

The `MessagesState` base class automatically includes a `messages` field for tracking conversation history.

### ResearchGraphState

```python
class ResearchGraphState(TypedDict):
    topic: str # Research topic
    max_analysts: int # Number of analysts
    human_analyst_feedback: str # Human feedback
    analysts: List[Analyst] # Analyst asking questions
    sections: Annotated[list, operator.add] # Send() API key
    introduction: str # Introduction for the final report
    content: str # Content for the final report
    conclusion: str # Conclusion for the final report
    final_report: str # Final report
```

This state type tracks information for the overall research process:
- All the fields from `GenerateAnalystsState`
- Accumulated sections from all interviews
- Introduction, content, conclusion, and final report text

## Designing Composable States

Our state types are designed to be composable, with information flowing from one state type to another. This composition happens in several ways:

### State Subsetting

Some nodes only need access to a subset of the overall state. For example, the `create_analysts` node operates on `GenerateAnalystsState`, which is a subset of `ResearchGraphState`:

```python
def create_analysts(state: GenerateAnalystsState):
    # ...
```

This subsetting helps clarify what each node needs and reduces the risk of inadvertently modifying unrelated parts of the state.

### State Transformation

The `initiate_all_interviews` function transforms parts of the outer state into inputs for the inner state:

```python
def initiate_all_interviews(state: ResearchGraphState):
    # ...
    return [
        Send("conduct_interview", {
            "analyst": analyst,
            "messages": [
                HumanMessage(
                    content=f"So you said you were writing an article on {topic}?"
                )
            ]
        }) for analyst in state["analysts"]
    ]
```

This function takes the `analysts` from the outer state and creates a separate interview state for each analyst, demonstrating how information flows from the outer state to the inner state.

### State Aggregation

Conversely, the `sections` field in `ResearchGraphState` aggregates results from multiple interviews:

```python
sections: Annotated[list, operator.add]
```

The `operator.add` annotation tells LangGraph to combine results from multiple executions of the `conduct_interview` node, demonstrating how information flows from the inner state back to the outer state.

## Annotated Fields and Operators

A key feature of our state design is the use of `Annotated` fields with operators:

```python
from typing import Annotated
import operator

class InterviewState(MessagesState):
    context: Annotated[list, operator.add] # Source docs
    # ...

class ResearchGraphState(TypedDict):
    sections: Annotated[list, operator.add] # Send() API key
    # ...
```

The `Annotated` type allows us to attach metadata to a type, and in LangGraph, this metadata can specify how values should be combined when a node returns a new value for a field.

The `operator.add` annotation tells LangGraph to use the `+` operator to combine the existing value with the new value. For lists, this means concatenation, which is exactly what we want for accumulating context and sections.

This is particularly important for parallel execution, where multiple interviews are running simultaneously and each generating its own sections. The `operator.add` annotation ensures that all these sections are collected and combined in the outer state.

## Working with MessagesState

The `InterviewState` extends LangGraph's `MessagesState`, which is a specialized state type for managing conversation histories:

```python
class InterviewState(MessagesState):
    max_num_turns: int # Number turns of conversation
    context: Annotated[list, operator.add] # Source docs
    analyst: Analyst # Analyst asking questions
    interview: str # Interview transcript
    sections: list # Final key we duplicate in outer state for Send() API
```

The `MessagesState` base class provides a `messages` field that automatically tracks the conversation history between the analyst and the expert. This field is used by the question generation and answer generation nodes to maintain context throughout the conversation.

## State Access Patterns

Different nodes access the state in different ways, depending on their needs:

### Dictionary Access

Many nodes use dictionary-style access to get and set state values:

```python
topic = state["topic"]
max_analysts = state["max_analysts"]
human_analyst_feedback = state.get('human_analyst_feedback', '')
```

The `get` method allows for providing a default value if the key doesn't exist, which is useful for optional fields like `human_analyst_feedback`.

### Return Values

Nodes return dictionaries containing only the fields they want to update:

```python
return {"analysts": analysts.analysts}
```

This follows LangGraph's immutable state pattern, where nodes don't modify the state directly but return changes to be applied.

### Nested Access

Some nodes need to access nested structures within the state:

```python
messages = state["messages"]
last_question = messages[-2]
```

This pattern is common when working with conversation histories, where we need to examine specific messages to determine what to do next.

## State Transitions and Patterns

Our state design follows several important patterns that facilitate smooth state transitions through the graph:

### Initialization Pattern

The system assumes an initial state containing at least the research topic and maximum number of analysts:

```python
# Example initial state
{
    "topic": "Climate change adaptation strategies",
    "max_analysts": 3
}
```

Subsequent nodes add more fields to this state as the workflow progresses.

### Accumulation Pattern

For fields that accumulate values from multiple sources, we use the `Annotated` pattern with `operator.add`:

```python
context: Annotated[list, operator.add]
sections: Annotated[list, operator.add]
```

This ensures that values are combined rather than replaced when multiple nodes update the same field.

### Transformation Pattern

Some nodes transform one part of the state into another:

```python
def save_interview(state: InterviewState):
    """ Save interviews """
    # Get messages
    messages = state["messages"]
    # Convert interview to a string
    interview = get_buffer_string(messages)
    # Save to interviews key
    return {"interview": interview}
```

This node transforms the `messages` list into a string representation stored in the `interview` field.

### Parallel Execution Pattern

The `Send()` API facilitates parallel execution by creating multiple instances of a subgraph, each with its own state:

```python
return [
    Send("conduct_interview", {
        "analyst": analyst,
        "messages": [
            HumanMessage(
                content=f"So you said you were writing an article on {topic}?"
            )
        ]
    }) for analyst in state["analysts"]
]
```

Each analyst gets its own interview state, and the results are aggregated back into the main state through the `sections` field.

## Ensuring State Consistency

Maintaining state consistency across complex workflows is challenging. Our design incorporates several features to help ensure consistency:

### Type Annotations

All state types have explicit type annotations:

```python
class ResearchGraphState(TypedDict):
    topic: str
    max_analysts: int
    # ...
```

These annotations help catch type errors during development and provide documentation for developers.

### Default Values

For optional fields, we use `get` with default values:

```python
human_analyst_feedback = state.get('human_analyst_feedback', '')
```

This ensures that even if a field is missing, the node can still function correctly.

### Explicit State Transitions

Every state transition is explicitly defined through node return values:

```python
return {"analysts": analysts.analysts}
```

This makes it clear what changes each node makes to the state, helping prevent unexpected state mutations.

### State Isolation

Each interview runs in its own isolated state, preventing interference between parallel executions:

```python
Send("conduct_interview", {
    "analyst": analyst,
    "messages": [...]
})
```

This isolation ensures that one interview doesn't affect another, even though they're running concurrently.

## Summary

In this chapter, we've explored the schema design and data modeling aspects of our LangGraph research assistant:

1. **Pydantic Models**: Provide structured data with validation and self-documentation
2. **TypedDict Classes**: Define the structure of state flowing through the graph
3. **MessagesState**: Specialized state type for managing conversation histories
4. **Annotated Fields**: Enable accumulation of values from multiple sources
5. **State Access Patterns**: Dictionary access, return values, and nested access
6. **State Transition Patterns**: Initialization, accumulation, transformation, and parallel execution
7. **State Consistency**: Type annotations, default values, explicit transitions, and state isolation

Effective schema design and data modeling are crucial for managing complex workflows like our research assistant. By carefully structuring our data and defining clear patterns for state transitions, we create a system that is both powerful and maintainable.

In the next chapter, we'll dive deeper into the process of creating AI analyst personas, exploring how we use LLMs to generate diverse perspectives on research topics.

[Continue...](ch04.md)
