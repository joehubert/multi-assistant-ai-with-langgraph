[TOC](ch00_overview.md)

# Chapter 2: System Architecture and Components

> "Architecture is the thoughtful making of space." — Louis Kahn

The LangGraph research assistant system is a sophisticated architecture that orchestrates multiple AI agents to produce comprehensive research reports. In this chapter, we'll examine the system's architecture and components, providing a foundation for the deeper dives in subsequent chapters.

## High-Level Architecture Overview

At its core, our research assistant is a nested graph structure that manages both sequential and parallel processes:

```
ResearchGraphState (Outer Graph)
  ├── create_analysts
  ├── human_feedback (optional interruption)
  ├── conduct_interview (Inner Graph) [Parallel execution]
  │    ├── ask_question
  │    ├── search_web / search_wikipedia
  │    ├── answer_question
  │    └── write_section
  ├── write_report
  ├── write_introduction
  ├── write_conclusion
  └── finalize_report
```

This structure enables a workflow that mirrors human research teams:

1. Multiple analyst personas are created, each focusing on different aspects of the research topic
2. Each analyst conducts an "interview" with an "expert" (which actually involves retrieving information)
3. Information from these interviews is synthesized into structured sections
4. These sections are compiled into a cohesive final report

What makes this architecture powerful is its ability to:
- Maintain state across complex workflows
- Support parallel execution where appropriate
- Allow for human intervention at strategic points
- Compose nested graphs for modular functionality

## Understanding StateGraph and MessagesState

The foundation of our architecture is LangGraph's `StateGraph` class, which provides the framework for defining and executing directed state flow graphs. Let's look at how we use it:

```python
# Outer graph
builder = StateGraph(ResearchGraphState)

# Inner graph (interview process)
interview_builder = StateGraph(InterviewState)
```

Each `StateGraph` is associated with a state type that defines the structure of the data flowing through the graph. In our case, we use:

1. `ResearchGraphState`: For the outer graph, tracking the overall research project
2. `InterviewState`: For the inner graph, tracking individual interviews

The `InterviewState` extends LangGraph's `MessagesState`, which is specifically designed for managing conversation histories. This is crucial for the interview process, where we need to track the back-and-forth between the "analyst" and the "expert."

```python
class InterviewState(MessagesState):
    max_num_turns: int # Number turns of conversation
    context: Annotated[list, operator.add] # Source docs
    analyst: Analyst # Analyst asking questions
    interview: str # Interview transcript
    sections: list # Final key we duplicate in outer state for Send() API
```

The `MessagesState` handles the `messages` field automatically, which stores the conversation history that our LLMs can access.

## Key Components

### Schema Definitions

Our system defines several Pydantic models to structure data flow:

```python
class Analyst(BaseModel):
    affiliation: str
    name: str
    role: str
    description: str
    
class Perspectives(BaseModel):
    analysts: List[Analyst]
    
class GenerateAnalystsState(TypedDict):
    topic: str
    max_analysts: int
    human_analyst_feedback: str
    analysts: List[Analyst]
    
class InterviewState(MessagesState):
    max_num_turns: int
    context: Annotated[list, operator.add]
    analyst: Analyst
    interview: str
    sections: list
    
class SearchQuery(BaseModel):
    search_query: str
    
class ResearchGraphState(TypedDict):
    topic: str
    max_analysts: int
    human_analyst_feedback: str
    analysts: List[Analyst]
    sections: Annotated[list, operator.add]
    introduction: str
    content: str
    conclusion: str
    final_report: str
```

These models provide type safety and structure, ensuring that data flows through our system in a consistent format.

### Node Functions

Nodes are the workhorses of our graph, implementing the actual logic that processes information. Each node is a Python function that:

1. Receives the current state
2. Performs some operation (often involving an LLM call)
3. Returns updates to the state

For example, here's the `create_analysts` node:

```python
def create_analysts(state: GenerateAnalystsState):
    """ Create analysts """
    
    topic = state['topic']
    max_analysts = state['max_analysts']
    human_analyst_feedback = state.get('human_analyst_feedback', '')
        
    # Enforce structured output
    structured_llm = llm.with_structured_output(Perspectives)

    # System message
    system_message = analyst_instructions.format(
        topic=topic,
        human_analyst_feedback=human_analyst_feedback, 
        max_analysts=max_analysts
    )

    # Generate question 
    analysts = structured_llm.invoke([
        SystemMessage(content=system_message),
        HumanMessage(content="Generate the set of analysts.")
    ])
    
    # Write the list of analysis to state
    return {"analysts": analysts.analysts}
```

This node takes the current state, calls an LLM to generate analyst personas, and returns these analysts to be added to the state.

### Edge Management

Edges define the flow between nodes, determining what happens next in the execution. Our system uses both static and conditional edges:

```python
# Static edge
builder.add_edge(START, "create_analysts")

# Conditional edges
builder.add_conditional_edges(
    "human_feedback", 
    initiate_all_interviews, 
    ["create_analysts", "conduct_interview"]
)
```

Static edges always lead to the same next node, while conditional edges determine the next node based on a function that examines the current state. This allows for dynamic workflow paths that adapt to the current context.

### State Handling

State in LangGraph is immutable, meaning nodes don't modify the existing state directly. Instead, each node returns a dictionary of changes to be applied to the state. This immutability ensures clean, predictable state transitions.

For accumulating data, we use `Annotated` fields with operators:

```python
context: Annotated[list, operator.add]
sections: Annotated[list, operator.add]
```

The `operator.add` annotation tells LangGraph to combine new values with existing ones rather than replacing them. This is crucial for accumulating context and sections across parallel executions.

## The Role of Send() API for Parallel Processing

One of the most powerful features of our architecture is its ability to run multiple interviews in parallel. This is accomplished through LangGraph's `Send()` API:

```python
def initiate_all_interviews(state: ResearchGraphState):
    # Check if human feedback
    human_analyst_feedback = state.get('human_analyst_feedback', 'approve')
    if human_analyst_feedback.lower() != 'approve':
        # Return to create_analysts
        return "create_analysts"
    
    # Otherwise kick off interviews in parallel via Send() API
    else:
        topic = state["topic"]
        return [
            Send("conduct_interview", {
                "analyst": analyst,
                "messages": [
                    HumanMessage(
                        content=f"So you said you were writing an article on {topic}?"
                    )
                ]
            }) for analyst in state["analysts"]
        ]
```

The `Send()` API allows us to execute the same node multiple times with different inputs. In this case, we're sending each analyst to conduct its own interview, with the results being collected and merged back into the main state.

This parallelization is a key efficiency feature, allowing multiple research streams to progress simultaneously rather than sequentially.

## Nested Graph Architecture

Our system uses a nested graph architecture, where the outer graph (`ResearchGraphState`) contains an inner graph (`InterviewState`) as one of its nodes:

```python
# Inner graph
interview_builder = StateGraph(InterviewState)
# ... define inner graph nodes and edges ...
interview_graph = interview_builder.compile()

# Outer graph
builder = StateGraph(ResearchGraphState)
builder.add_node("conduct_interview", interview_graph)
```

This nesting allows for modular, reusable components that encapsulate specific functionalities. In our case, the interview process is a self-contained workflow that can be executed multiple times within the larger research process.

## Human-in-the-Loop Design

Our architecture incorporates human-in-the-loop capabilities through interruption points:

```python
# Compile with interruption
graph = builder.compile(interrupt_before=['human_feedback'])
```

The `interrupt_before` parameter specifies nodes where execution should pause and wait for human input. In our case, the system can generate analyst personas and then pause to let a human review and potentially modify them before proceeding.

This design acknowledges that while AI can automate much of the research process, human judgment and expertise remain valuable, especially for guiding the direction of research.

## LLM Configuration

Our system uses OpenAI's GPT-4 model as its language model backend:

```python
llm = ChatOpenAI(model="gpt-4o", temperature=0)
```

The `temperature=0` setting ensures deterministic outputs, which is generally preferable for structured tasks like ours where consistency is more important than creativity.

For structured outputs, we adapt this LLM with structured output validation:

```python
structured_llm = llm.with_structured_output(Perspectives)
```

This ensures that LLM outputs conform to our predefined schemas, making the system more robust against malformed responses.

## Information Retrieval Tools

Our system incorporates two main information retrieval tools:

```python
# Web search
tavily_search = TavilySearchResults(max_results=3)

# Wikipedia search
search_docs = WikipediaLoader(
    query=search_query.search_query, 
    load_max_docs=2
).load()
```

These tools allow our system to access up-to-date information from the web and Wikipedia, providing factual grounding for the research process.

The retrieved information is formatted into a consistent structure and added to the interview context, where it can be referenced by the "expert" when answering questions.

## Summary

In this chapter, we've explored the architecture and components of our LangGraph research assistant system:

1. The overall architecture uses nested graphs for modularity and parallel execution
2. State is managed through typed Pydantic models and TypedDict classes
3. Node functions implement the core logic, often involving LLM calls
4. Edges define the flow between nodes, with conditional routing for dynamic paths
5. The Send() API enables parallel execution of multiple interviews
6. Human-in-the-loop capability is provided through interruption points
7. Information retrieval tools ground the system in factual knowledge

This architecture demonstrates the power of LangGraph for orchestrating complex AI workflows. By breaking down the research process into discrete, specialized components, we create a system that is both powerful and maintainable.

In the next chapter, we'll dive deeper into the schema design and data modeling aspects of our system, exploring how we structure and manage information throughout the research process.

[Continue...](ch03.md)
