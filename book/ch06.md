# Chapter 6: Document Retrieval and Context Management

> "Information is the oil of the 21st century, and analytics is the combustion engine." â€” Peter Sondergaard

A research assistant is only as good as the information it can access. In this chapter, we'll explore how our LangGraph research assistant retrieves and manages information from external sources, creating a factual foundation for the research process.

## Working with TavilySearchResults

Our primary tool for web search is LangChain's `TavilySearchResults` integration:

```python
from langchain_community.tools.tavily_search import TavilySearchResults

def search_web(state: InterviewState):
    """ Retrieve docs from web search """

    # Search
    tavily_search = TavilySearchResults(max_results=3)

    # Search query
    structured_llm = llm.with_structured_output(SearchQuery)
    search_query = structured_llm.invoke([search_instructions]+state['messages'])
    
    # Search
    search_docs = tavily_search.invoke(search_query.search_query)

     # Format
    formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document href="{doc["url"]}"/>\n{doc["content"]}\n</Document>'
            for doc in search_docs
        ]
    )

    return {"context": [formatted_search_docs]} 
```

Tavily is a search API specifically designed for AI applications, offering several advantages:

1. **Relevance**: Results are optimized for relevance to the query
2. **Summarization**: Content is pre-summarized to fit the context windows of LLMs
3. **Metadata**: Results include useful metadata like URLs
4. **API Design**: The API is designed for easy integration with LangChain

Our implementation uses the following workflow:

1. **Query Generation**: Convert the conversation into a search query using an LLM
2. **API Call**: Retrieve up to 3 documents using the Tavily API
3. **Formatting**: Structure the results with source URLs and content
4. **Context Addition**: Add the formatted documents to the interview context

### Maximum Results

We limit the search to 3 results (`max_results=3`) to balance comprehensiveness with efficiency. This limit:

1. Ensures diverse perspectives by retrieving multiple sources
2. Avoids overwhelming the context with too much information
3. Keeps API usage and costs reasonable

For most research queries, 3 high-quality results provide sufficient information while maintaining efficiency.

## Implementing WikipediaLoader

In addition to web search, our system can retrieve information from Wikipedia using LangChain's `WikipediaLoader`:

```python
from langchain_community.document_loaders import WikipediaLoader

def search_wikipedia(state: InterviewState):
    """ Retrieve docs from wikipedia """

    # Search query
    structured_llm = llm.with_structured_output(SearchQuery)
    search_query = structured_llm.invoke([search_instructions]+state['messages'])
    
    # Search
    search_docs = WikipediaLoader(query=search_query.search_query, 
                              load_max_docs=2).load()

     # Format
    formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document source="{doc.metadata["source"]}" page="{doc.metadata.get("page", "")}"/>\n{doc.page_content}\n</Document>'
            for doc in search_docs
        ]
    )

    return {"context": [formatted_search_docs]} 
```

Wikipedia is an excellent source for research for several reasons:

1. **Breadth**: It covers a vast range of topics
2. **Structure**: Content is well-organized and consistently formatted
3. **Citations**: Most articles include citations to primary sources
4. **Neutrality**: Content aims for a neutral point of view

Our implementation follows a similar workflow to the web search:

1. **Query Generation**: Convert the conversation into a search query
2. **Wikipedia API Call**: Retrieve up to 2 articles from Wikipedia
3. **Formatting**: Structure the results with source information
4. **Context Addition**: Add the formatted articles to the interview context

### Load Maximum Documents

We limit the Wikipedia search to 2 articles (`load_max_docs=2`) since Wikipedia articles tend to be longer and more comprehensive than typical web search results. This limit ensures that we get sufficient information without overwhelming the context.

## Formatting Search Documents

Both retrieval methods use a consistent formatting approach for the retrieved documents:

```python
formatted_search_docs = "\n\n---\n\n".join(
    [
        f'<Document href="{doc["url"]}"/>\n{doc["content"]}\n</Document>'
        for doc in search_docs
    ]
)
```

For web search, and:

```python
formatted_search_docs = "\n\n---\n\n".join(
    [
        f'<Document source="{doc.metadata["source"]}" page="{doc.metadata.get("page", "")}"/>\n{doc.page_content}\n</Document>'
        for doc in search_docs
    ]
)
```

For Wikipedia search.

This formatting serves several important purposes:

1. **Document Boundaries**: Clearly delineates where one document ends and another begins
2. **Source Attribution**: Includes source URLs or Wikipedia article names
3. **Metadata**: Includes additional metadata like page numbers where available
4. **XML Structure**: Uses XML-like tags that are easy for LLMs to parse

The consistent structure helps the LLM understand and cite the sources correctly when generating expert answers.

## Context Accumulation with Annotated Lists

As the interview progresses, the system accumulates context from multiple searches. This accumulation is managed through the `context` field in the interview state:

```python
context: Annotated[list, operator.add]
```

The `Annotated` type with `operator.add` tells LangGraph to combine new context with existing context rather than replacing it. This means that as the interview progresses and more searches are performed, the context grows to include all retrieved information.

This accumulation is crucial for building a comprehensive understanding of the research topic. Each question-answer pair can add new information to the context, allowing subsequent questions to build on previous information.

## Citations and Source Management

Proper citation of sources is a critical aspect of research. Our system includes several features to ensure accurate attribution:

### Source Formatting

Sources are formatted consistently in the context:

```python
<Document href="https://example.com/article"/>\nContent here\n</Document>
```

Or:

```python
<Document source="Wikipedia" page="Climate change"/>\nContent here\n</Document>
```

This format makes it easy for the LLM to identify and cite sources.

### Expert Answer Instructions

The system prompt for expert answers includes specific instructions for citation:

```python
3. The context contain sources at the topic of each individual document.

4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. 

5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc
```

These instructions ensure that the expert's answers include proper citations for all information.

### Report Section Instructions

Similarly, the section writer instructions include detailed guidance on source management:

```python
6. In the Sources section:
- Include all sources used in your report
- Provide full links to relevant websites or specific document paths
- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.
- It will look like:

### Sources
[1] Link or Document name
[2] Link or Document name

7. Be sure to combine sources. For example this is not correct:

[3] https://ai.meta.com/blog/meta-llama-3-1/
[4] https://ai.meta.com/blog/meta-llama-3-1/

There should be no redundant sources. It should simply be:

[3] https://ai.meta.com/blog/meta-llama-3-1/
```

These detailed instructions help ensure consistent and accurate citation throughout the research process.

## Balancing Context and Tokens

LLMs have context window limitations, meaning they can only process a certain number of tokens at once. Our system needs to balance comprehensive context with these limitations. Several features help manage this balance:

### Limited Search Results

By limiting the number of search results (3 for web search, 2 for Wikipedia), we ensure that the context doesn't grow too large too quickly.

### Structured Interview Flow

The structured interview flow, with a limited number of turns, helps prevent the context from becoming excessive.

### Citation Instructions

By including clear citation instructions, we ensure that information sources are tracked even as the context grows, maintaining attribution without unnecessary repetition.

## Search Query Optimization

The quality of search results depends heavily on the quality of the search queries. Our system uses an LLM to generate optimized search queries based on the conversation:

```python
search_instructions = SystemMessage(content=f"""You will be given a conversation between an analyst and an expert. 

Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.
        
First, analyze the full conversation.

Pay particular attention to the final question posed by the analyst.

Convert this final question into a well-structured web search query""")
```

This approach has several advantages:

1. **Contextual Understanding**: The LLM can understand the broader context of the conversation, not just the latest question
2. **Query Reformulation**: The LLM can reformulate questions into more effective search queries
3. **Key Term Identification**: The LLM can identify and emphasize key terms from the conversation

For example, if an analyst asks "How has this approach evolved over the past decade?", the LLM might generate a query like "evolution development progress [topic] last 10 years", which is more likely to yield relevant search results.

## Alternative Retrieval Paths

Our system includes two separate retrieval paths - web search and Wikipedia - that run in parallel:

```python
interview_builder.add_edge("ask_question", "search_web")
interview_builder.add_edge("ask_question", "search_wikipedia")
interview_builder.add_edge("search_web", "answer_question")
interview_builder.add_edge("search_wikipedia", "answer_question")
```

This parallel approach has several benefits:

1. **Source Diversity**: Web search and Wikipedia often provide different types of information
2. **Robustness**: If one search method fails or returns poor results, the other can compensate
3. **Complementary Information**: The two sources often complement each other, with Wikipedia providing background and web search providing current developments

The context from both sources is combined and used to generate the expert's answer, providing a more comprehensive foundation for the research.

## Handling Search Failures

In real-world applications, searches don't always return useful results. Our system needs to handle these failures gracefully. Several features help with this:

### Context Accumulation

By accumulating context throughout the interview, the system ensures that even if one search fails, the expert still has previous context to draw on.

### Expert Prompt Design

The expert prompt includes instructions to use only the provided context:

```python
1. Use only the information provided in the context. 
        
2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.
```

If the context is insufficient, the expert should acknowledge this rather than inventing information, maintaining the integrity of the research process.

### Multiple Turns

The multi-turn interview format allows the analyst to ask follow-up questions if the initial answers are insufficient, potentially triggering new searches that yield better results.

## Document Schema Standardization

Both web search and Wikipedia results are converted to a standardized document schema before being added to the context:

```python
<Document [metadata]/>\n[content]\n</Document>
```

This standardization has several advantages:

1. **Consistent Parsing**: The LLM can parse documents using the same approach regardless of their source
2. **Metadata Flexibility**: Different sources can include different metadata (URLs, page numbers, etc.)
3. **Clear Boundaries**: The structure clearly indicates where documents begin and end

This standardized schema helps ensure consistent handling of information regardless of its source.

## Context Persistence Across Turns

As the interview progresses through multiple question-answer turns, the context persists and grows. This persistence is crucial for building a coherent understanding of the research topic.

For example, if Turn 1 establishes basic concepts and Turn 2 explores applications, the expert in Turn 2 has access to both the application information from the latest search and the conceptual foundation from the previous turn. This allows for more sophisticated and contextually aware responses.

## Summary

In this chapter, we've explored the document retrieval and context management aspects of our LangGraph research assistant:

1. **Tavily Search**: Web search optimized for AI applications
2. **Wikipedia Loader**: Structured knowledge retrieval from Wikipedia
3. **Document Formatting**: Consistent structure for source attribution
4. **Context Accumulation**: Building comprehensive context through `Annotated` fields
5. **Citation Management**: Ensuring proper attribution throughout the research process
6. **Query Optimization**: Using LLMs to generate effective search queries
7. **Parallel Retrieval**: Combining information from multiple sources
8. **Failure Handling**: Gracefully managing search failures
9. **Schema Standardization**: Consistent document structure across sources
10. **Context Persistence**: Maintaining information across multiple turns

Effective information retrieval and context management are foundational to the research process. By implementing robust systems for gathering, structuring, and citing information, our research assistant can produce well-informed and properly attributed research reports.

In the next chapter, we'll explore the section writing and report generation process, examining how the system transforms information into coherent narrative.
