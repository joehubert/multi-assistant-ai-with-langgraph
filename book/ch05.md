# Chapter 5: Building the Interview System

> "The art of the interviewer is to extract what he wants to know from someone who may not wish to tell him. To persuade, cajole, and enlighten all at once." — Barbara Walters

At the heart of our research assistant is the interview system—a sophisticated workflow that simulates conversations between analyst personas and "experts" to gather information on research topics. This chapter explores how we implement this system, from question generation to answer synthesis, and how we manage the conversation flow throughout.

## The Interview State Machine

The interview process is implemented as a separate state graph using LangGraph's nested graph capabilities:

```python
interview_builder = StateGraph(InterviewState)
interview_builder.add_node("ask_question", generate_question)
interview_builder.add_node("search_web", search_web)
interview_builder.add_node("search_wikipedia", search_wikipedia)
interview_builder.add_node("answer_question", generate_answer)
interview_builder.add_node("save_interview", save_interview)
interview_builder.add_node("write_section", write_section)

# Flow
interview_builder.add_edge(START, "ask_question")
interview_builder.add_edge("ask_question", "search_web")
interview_builder.add_edge("ask_question", "search_wikipedia")
interview_builder.add_edge("search_web", "answer_question")
interview_builder.add_edge("search_wikipedia", "answer_question")
interview_builder.add_conditional_edges("answer_question", route_messages,['ask_question','save_interview'])
interview_builder.add_edge("save_interview", "write_section")
interview_builder.add_edge("write_section", END)
```

This graph defines a workflow with the following steps:

1. Generate a question from the analyst persona
2. Search for information (via web or Wikipedia)
3. Generate an answer based on the retrieved information
4. Either ask another question or end the interview
5. Save the interview transcript
6. Write a section based on the interview

The state type for this graph is `InterviewState`, which extends `MessagesState` to manage the conversation history:

```python
class InterviewState(MessagesState):
    max_num_turns: int # Number turns of conversation
    context: Annotated[list, operator.add] # Source docs
    analyst: Analyst # Analyst asking questions
    interview: str # Interview transcript
    sections: list # Final key we duplicate in outer state for Send() API
```

This state tracks not only the conversation messages but also accumulated context from searches, the analyst persona, and the final interview transcript and section.

## Question Generation Techniques

The first step in the interview process is generating questions that reflect the analyst's persona and interests. This is implemented in the `generate_question` node:

```python
def generate_question(state: InterviewState):
    """ Node to generate a question """

    # Get state
    analyst = state["analyst"]
    messages = state["messages"]

    # Generate question 
    system_message = question_instructions.format(goals=analyst.persona)
    question = llm.invoke([SystemMessage(content=system_message)]+messages)
        
    # Write messages to state
    return {"messages": [question]}
```

This node takes the analyst persona and current conversation history as inputs, and generates a question as output using a system prompt:

```python
question_instructions = """You are an analyst tasked with interviewing an expert to learn about a specific topic. 

Your goal is boil down to interesting and specific insights related to your topic.

1. Interesting: Insights that people will find surprising or non-obvious.
        
2. Specific: Insights that avoid generalities and include specific examples from the expert.

Here is your topic of focus and set of goals: {goals}
        
Begin by introducing yourself using a name that fits your persona, and then ask your question.

Continue to ask questions to drill down and refine your understanding of the topic.
        
When you are satisfied with your understanding, complete the interview with: "Thank you so much for your help!"

Remember to stay in character throughout your response, reflecting the persona and goals provided to you."""
```

This prompt instructs the LLM to:

1. Adopt the analyst's persona
2. Ask questions that seek interesting and specific insights
3. Drill down to refine understanding
4. Signal completion when satisfied
5. Stay in character throughout

The resulting questions reflect the analyst's specific perspective and interests, directing the information gathering process toward relevant aspects of the research topic.

## Information Retrieval Strategies

Once a question is generated, the system needs to gather information to answer it. Our implementation uses two main information retrieval strategies:

### Web Search Implementation

The `search_web` node uses the Tavily Search API to retrieve information from the web:

```python
def search_web(state: InterviewState):
    """ Retrieve docs from web search """

    # Search
    tavily_search = TavilySearchResults(max_results=3)

    # Search query
    structured_llm = llm.with_structured_output(SearchQuery)
    search_query = structured_llm.invoke([search_instructions]+state['messages'])
    
    # Search
    search_docs = tavily_search.invoke(search_query.search_query)

     # Format
    formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document href="{doc["url"]}"/>\n{doc["content"]}\n</Document>'
            for doc in search_docs
        ]
    )

    return {"context": [formatted_search_docs]} 
```

This node:

1. Converts the conversation into a search query using an LLM
2. Retrieves up to 3 documents from the web using Tavily Search
3. Formats the retrieved documents with source URLs
4. Adds the formatted documents to the context

### Wikipedia Retrieval

The `search_wikipedia` node uses LangChain's WikipediaLoader to retrieve information from Wikipedia:

```python
def search_wikipedia(state: InterviewState):
    """ Retrieve docs from wikipedia """

    # Search query
    structured_llm = llm.with_structured_output(SearchQuery)
    search_query = structured_llm.invoke([search_instructions]+state['messages'])
    
    # Search
    search_docs = WikipediaLoader(query=search_query.search_query, 
                              load_max_docs=2).load()

     # Format
    formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document source="{doc.metadata["source"]}" page="{doc.metadata.get("page", "")}"/>\n{doc.page_content}\n</Document>'
            for doc in search_docs
        ]
    )

    return {"context": [formatted_search_docs]} 
```

This node follows a similar pattern to the web search, but uses Wikipedia as its information source.

### Search Query Generation

Both retrieval strategies use a common system prompt to generate search queries:

```python
search_instructions = SystemMessage(content=f"""You will be given a conversation between an analyst and an expert. 

Your goal is to generate a well-structured query for use in retrieval and / or web-search related to the conversation.
        
First, analyze the full conversation.

Pay particular attention to the final question posed by the analyst.

Convert this final question into a well-structured web search query""")
```

This prompt instructs the LLM to:

1. Analyze the full conversation
2. Focus on the most recent question
3. Convert this question into a search query

The resulting query is then used to retrieve relevant information from the web or Wikipedia.

## Implementing Expert Answers

Once information is retrieved, the system generates an "expert" answer based on this information. This is implemented in the `generate_answer` node:

```python
def generate_answer(state: InterviewState):
    """ Node to answer a question """

    # Get state
    analyst = state["analyst"]
    messages = state["messages"]
    context = state["context"]

    # Answer question
    system_message = answer_instructions.format(goals=analyst.persona, context=context)
    answer = llm.invoke([SystemMessage(content=system_message)]+messages)
            
    # Name the message as coming from the expert
    answer.name = "expert"
    
    # Append it to state
    return {"messages": [answer]}
```

This node takes the analyst persona, conversation history, and retrieved context as inputs, and generates an answer as output using a system prompt:

```python
answer_instructions = """You are an expert being interviewed by an analyst.

Here is analyst area of focus: {goals}. 
        
You goal is to answer a question posed by the interviewer.

To answer question, use this context:
        
{context}

When answering questions, follow these guidelines:
        
1. Use only the information provided in the context. 
        
2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.

3. The context contain sources at the topic of each individual document.

4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. 

5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc
        
6. If the source is: <Document source="assistant/docs/llama3_1.pdf" page="7"/>' then just list: 
        
[1] assistant/docs/llama3_1.pdf, page 7 
        
And skip the addition of the brackets as well as the Document source preamble in your citation."""
```

This prompt instructs the LLM to:

1. Adopt an expert persona
2. Answer the analyst's question using only the provided context
3. Include proper citations for information sources
4. Format sources consistently

The resulting answer simulates an expert response grounded in the retrieved information, creating a realistic interview experience.

## Tracking Conversation Turns

The interview system needs to track the conversation and determine when to end the interview. This is implemented in the `route_messages` function:

```python
def route_messages(state: InterviewState, 
               name: str = "expert"):
    """ Route between question and answer """
    
    # Get messages
    messages = state["messages"]
    max_num_turns = state.get('max_num_turns',2)

    # Check the number of expert answers 
    num_responses = len(
        [m for m in messages if isinstance(m, AIMessage) and m.name == name]
    )

    # End if expert has answered more than the max turns
    if num_responses >= max_num_turns:
        return 'save_interview'

    # This router is run after each question - answer pair 
    # Get the last question asked to check if it signals the end of discussion
    last_question = messages[-2]
    
    if "Thank you so much for your help" in last_question.content:
        return 'save_interview'
    return "ask_question"
```

This function makes routing decisions based on two criteria:

1. The number of expert responses: If the expert has answered more than `max_num_turns` questions, the interview ends.
2. Explicit ending: If the analyst's last question contains "Thank you so much for your help!" (the signal phrase from the question prompt), the interview ends.

Otherwise, the interview continues with another question.

This dual approach ensures that interviews don't run indefinitely but also allows analysts to end early if they've gathered enough information.

## Router Logic for Conversation Flow

The interview graph uses conditional edges to implement the routing logic:

```python
interview_builder.add_conditional_edges("answer_question", route_messages,['ask_question','save_interview'])
```

This line specifies that after the `answer_question` node, the graph should call the `route_messages` function to determine whether to proceed to the `ask_question` node (continuing the interview) or the `save_interview` node (ending the interview).

### Saving the Interview

When the interview ends, the `save_interview` node converts the message history into a string transcript:

```python
def save_interview(state: InterviewState):
    """ Save interviews """

    # Get messages
    messages = state["messages"]
    
    # Convert interview to a string
    interview = get_buffer_string(messages)
    
    # Save to interviews key
    return {"interview": interview}
```

This transcript can then be used in the subsequent section writing process.

## Section Writing Process

The final step in the interview process is writing a structured section based on the interview. This is implemented in the `write_section` node:

```python
def write_section(state: InterviewState):
    """ Node to write a section """

    # Get state
    interview = state["interview"]
    context = state["context"]
    analyst = state["analyst"]
   
    # Write section using either the gathered source docs from interview (context) or the interview itself (interview)
    system_message = section_writer_instructions.format(focus=analyst.description)
    section = llm.invoke([SystemMessage(content=system_message)]+[HumanMessage(content=f"Use this source to write your section: {context}")]) 
                
    # Append it to state
    return {"sections": [section.content]}
